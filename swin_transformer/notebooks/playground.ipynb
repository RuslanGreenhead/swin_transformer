{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPCeX3MEi_0g",
        "outputId": "d8f4efbb-edea-4887-e35a-5f7bedd21f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kora -q\n",
        "from kora import drive\n",
        "drive.link_nbs()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/AI_Notebooks/DLS_YSDA/Diffusion/UNetDiffusion')"
      ],
      "metadata": {
        "id": "OaNVxxMVjQ8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0) Data issues + patching + patch embeddings"
      ],
      "metadata": {
        "id": "zNMj8kKujXFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageNet, CIFAR10\n",
        "from torchvision import transforms\n",
        "from timm.layers import DropPath\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "from torch import einsum"
      ],
      "metadata": {
        "id": "kNMMg0iHjjkO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset + dataloader creation (ImageNet)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])    # why such values??\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = ImageNet('.', split='train', transform=transform_train)\n",
        "val_dataset = ImageNet('.', split='val', transform=transform_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "iNmY3zP4kOxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset + dataloader creation (CIFAR10)\n",
        "\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize with mean and std for CIFAR-10\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='.', train=True, download=True, transform=transform_cifar)\n",
        "# val_dataset = CIFAR10(root='.', train=False, download=True, transform=transform_cifar)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M53U_PDm7DS",
        "outputId": "2b9b3527-35b3-42c0-e884-309c6b8ea33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 56.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_image = train_dataset[45][0]\n",
        "\n",
        "plt.imshow(tensor_image.permute(1, 2, 0));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "pjBWkofDq4ev",
        "outputId": "b0c221f0-04a8-48a9-8d9e-79ed000d6e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tens = torch.arange(48).view(4, 4, 3).to(torch.float)\n",
        "tens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wzDzxrMrAsO",
        "outputId": "bd24671a-1479-40e5-dc64-b7ff34578bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.,  1.,  2.],\n",
              "         [ 3.,  4.,  5.],\n",
              "         [ 6.,  7.,  8.],\n",
              "         [ 9., 10., 11.]],\n",
              "\n",
              "        [[12., 13., 14.],\n",
              "         [15., 16., 17.],\n",
              "         [18., 19., 20.],\n",
              "         [21., 22., 23.]],\n",
              "\n",
              "        [[24., 25., 26.],\n",
              "         [27., 28., 29.],\n",
              "         [30., 31., 32.],\n",
              "         [33., 34., 35.]],\n",
              "\n",
              "        [[36., 37., 38.],\n",
              "         [39., 40., 41.],\n",
              "         [42., 43., 44.],\n",
              "         [45., 46., 47.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_patches(image, patch_h, patch_w):\n",
        "    \"\"\"\n",
        "    Extracts (patch_h)x(patch_w) patches from the input image and flattens each patch.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.ndarray): Input image of shape (H, W, 3).\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Output array of shape (H/4, W/4, 48).\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the image has the correct shape\n",
        "    assert image.ndim == 3 and image.shape[2] == 3, \"Input must be an image with shape (H, W, 3)\"\n",
        "    # Use einops to rearrange the image into patches and flatten them\n",
        "    patches = rearrange(image, '(h h2) (w w2) c -> h w (c h2 w2)', h2=patch_h, w2=patch_w)\n",
        "\n",
        "    return patches"
      ],
      "metadata": {
        "id": "w1Z69-Ofup4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_patches(tens, 2, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuJg--L9vNxx",
        "outputId": "907977d3-6697-48fd-9641-fd4417543333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.,  3., 12., 15.,  1.,  4., 13., 16.,  2.,  5., 14., 17.],\n",
              "         [ 6.,  9., 18., 21.,  7., 10., 19., 22.,  8., 11., 20., 23.]],\n",
              "\n",
              "        [[24., 27., 36., 39., 25., 28., 37., 40., 26., 29., 38., 41.],\n",
              "         [30., 33., 42., 45., 31., 34., 43., 46., 32., 35., 44., 47.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InitLinear(nn.Module):\n",
        "    def __init__(self, emb_size=32):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features=12, out_features=emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # reshaping\n",
        "        reshaped_input = x.view(-1, 12)\n",
        "        # applying linear layer\n",
        "        output = self.linear(reshaped_input)\n",
        "        output = output.view(2, 2, self.emb_size)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "mikXkoRJvQtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_fc = InitLinear(emb_size=16)\n",
        "patched = make_patches(tens, 2, 2)\n",
        "res = init_fc(patched)\n",
        "\n",
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7XsD_orGUsB",
        "outputId": "a15dc98f-afce-48cf-9810-0497d8fe6301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_fc.linear.weight.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6D7vsuoIm0S",
        "outputId": "336ea98c-682a-48d9-8bee-dc4f389e96d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.arange(96).view(2, 2, 2, 12).to(torch.float)\n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OyB5V92MCm3",
        "outputId": "6cebb0dc-d637-4a73-e726-4b150e39782a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23.]],\n",
              "\n",
              "         [[24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
              "          [36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47.]]],\n",
              "\n",
              "\n",
              "        [[[48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59.],\n",
              "          [60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71.]],\n",
              "\n",
              "         [[72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.],\n",
              "          [84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = t.view(-1, 12)\n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9WQzIDJMzTA",
        "outputId": "4bfa1529-4e8b-473a-ae17-b46fee02d8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
              "        [12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23.],\n",
              "        [24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
              "        [36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47.],\n",
              "        [48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59.],\n",
              "        [60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71.],\n",
              "        [72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.],\n",
              "        [84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95.]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = t.view(2, 2, 2, 12)\n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqAtBTbvMntr",
        "outputId": "f6cc3eaa-c283-4eb3-a0db-b1f4a8c4b5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23.]],\n",
              "\n",
              "         [[24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
              "          [36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47.]]],\n",
              "\n",
              "\n",
              "        [[[48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59.],\n",
              "          [60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71.]],\n",
              "\n",
              "         [[72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.],\n",
              "          [84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) Patch Embedding"
      ],
      "metadata": {
        "id": "Paa8Ru1INIu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Splits input image into patches -> maps patches to embedding vectors\n",
        "\n",
        "    Parameters:\n",
        "        img_size (int): size of input image (supposed it comes in a square form)\n",
        "        patch_size (int): length of patch's side in pixels\n",
        "        emb_dim (int): dimension of aquired patch embeddings\n",
        "        norm_layer (nn.Module / None): normalization to apply afterwards\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, emb_dim=16, norm_layer=None):\n",
        "        super().__init__()\n",
        "        self.patch_conv = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=emb_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = (img_size // patch_size, img_size // patch_size)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.norm_layer = norm_layer(emb_dim) if norm_layer is not None else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) == 4, \"only batched 3d tensors supported\"\n",
        "        assert x.shape[3] == 3, \"input tensor has to be in (H, W, 3) format\"\n",
        "        assert x.shape[1] % self.patch_size == 0, \"tensor size has to be divisible by patch_size\"\n",
        "\n",
        "        permuted_x = x.permute(0, 3, 1, 2)      # to (bchw)\n",
        "        res = self.patch_conv(permuted_x)\n",
        "        res = res.permute(0, 2, 3, 1)           # back to (bhwc)\n",
        "        if self.norm_layer is not None:\n",
        "            res = self.norm_layer(res)\n",
        "\n",
        "        return res"
      ],
      "metadata": {
        "id": "rATIXfawMxJw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tens = torch.arange(48).view(4, 4, 3).to(torch.float).unsqueeze(0)\n",
        "tens = torch.randn((1, 32, 32, 3))\n",
        "tens.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVOH2Sy_PGlK",
        "outputId": "f04567a2-2ee9-440c-875b-7de92769d3b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PatchEmbedding(patch_size=4, emb_dim=16, norm_layer=nn.LayerNorm)\n",
        "\n",
        "p_embs = pe(tens)\n",
        "p_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svG3sGqLPM98",
        "outputId": "be021347-9fdf-434b-8bb4-9b71f85f5ce2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) Window partition + window merging"
      ],
      "metadata": {
        "id": "kPF83zwkVbKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etZcU2NhVup9",
        "outputId": "c32a8fd7-d21b-4705-bdf7-364d9dbfa465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size=4):\n",
        "    \"\"\"\n",
        "    Devides patched tensor into windows.\n",
        "\n",
        "    Parameters:\n",
        "        x (tensor): tensor of patches -> (B, H, W, C)\n",
        "        window_size (int): size of window\n",
        "\n",
        "    Returns:\n",
        "        tensor: tensor of windows -> (n * B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(x.shape) == 4, \"suppports only batched tensors\"\n",
        "    assert (x.shape[1] % window_size == 0) and (x.shape[2] % window_size == 0)\n",
        "\n",
        "    windows = rearrange(x, 'b (h h2) (w w2) c -> (b h w) h2 w2 c', h2=window_size, w2=window_size)\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_merging(x, window_size, h, w):\n",
        "    \"\"\"\n",
        "    Assembles tensor of windows into single patched tensor.\n",
        "\n",
        "    Parameters:\n",
        "        x (tensor): tensor of windows -> (n * B, window_size, window_size, C)\n",
        "        window_size (int): size of window\n",
        "\n",
        "    Returns:\n",
        "        tensor: tensor of patches -> (B, H, W, C)\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(x.shape) == 4, \"suppports only batched tensors\"\n",
        "    assert (x.shape[1] == window_size) and (x.shape[2] == window_size)\n",
        "\n",
        "    h_ = h // window_size\n",
        "    w_ = w // window_size\n",
        "\n",
        "    patches = rearrange(x, '(b h w) h2 w2 c -> b (h h2) (w w2) c', h=h_, w=w_)\n",
        "\n",
        "    return patches"
      ],
      "metadata": {
        "id": "oCVZQuxIdSba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check window partition shapes\n",
        "\n",
        "print(f\"{p_embs.shape=}\")\n",
        "wp_mine = window_partition(p_embs, 4)\n",
        "print(f\"{wp_mine.shape=}\")\n",
        "\n",
        "p_mine = window_merging(wp_mine, 4, 8, 8)\n",
        "print(f\"{p_mine.shape=}\")\n",
        "\n",
        "# tt = torch.arange(32).reshape(1, 4, 4, 2)\n",
        "# ttwp = window_partition(tt, 2)\n",
        "# print(f\"{ttwp.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djalSZttnrGw",
        "outputId": "3e81060f-bf2a-4dd8-9bcd-e7da673696bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_embs.shape=torch.Size([1, 8, 8, 16])\n",
            "wp_mine.shape=torch.Size([4, 4, 4, 16])\n",
            "p_mine.shape=torch.Size([1, 8, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравнение выходов моих функций с выходами функций из офригинальной реализации\n",
        "\n",
        "def _window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def _window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "t = torch.arange(32).reshape(1, 4, 4 , 2)\n",
        "wp_1 = window_partition(t, 2)\n",
        "wp_2 = _window_partition(t, 2)\n",
        "p1 = window_merging(wp_1, 2, 4, 4)\n",
        "p2 = _window_reverse(wp_1, 2, 4, 4)\n",
        "\n",
        "# --> все сходится"
      ],
      "metadata": {
        "id": "oFk2kcaRNXbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) (MLP + WMSA) -> SWIN block"
      ],
      "metadata": {
        "id": "hfxFL2aNRO5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron\n",
        "\n",
        "    Parameters:\n",
        "        in_features (int): number of input tensor's features\n",
        "        hid_features (int): number of neurons at hidden layer\n",
        "        out_features (int): number of neurons at output layer\n",
        "        act_layer (nn.Module): activation layer\n",
        "        drop (float): dropout proabbility -> (0, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hid_features=None, out_features=None,\n",
        "                 act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        hid_features = hid_features or in_features\n",
        "        out_features = out_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hid_features)\n",
        "        self.act_layer = act_layer()\n",
        "        self.fc2 = nn.Linear(hid_features, out_features)\n",
        "        self.drop_layer = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_layer(x)\n",
        "        x = self.drop_layer(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "XNGwoOkqWD-L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WMSA(nn.Module):\n",
        "    \"\"\"\n",
        "    Window-based Multi-head Self Attention\n",
        "\n",
        "    Parameters:\n",
        "        input _dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        n_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        output_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, window_size, n_heads,\n",
        "                 qkv_bias=True, qk_scale=None, attn_drop=0., output_drop=0.):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.window_size = window_size\n",
        "        self.n_heads = n_heads\n",
        "        assert input_dim % n_heads == 0    # -> to avoid errors in rearranging\n",
        "        self.head_dim = input_dim // n_heads\n",
        "        self.qk_scale = qk_scale or self.head_dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Conv2d(input_dim, self.n_heads * self.head_dim * 3, 1, bias=qkv_bias)\n",
        "        self.to_out = nn.Conv2d(self.n_heads * self.head_dim, input_dim, 1)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.output_drop = nn.Dropout(output_drop)\n",
        "\n",
        "        # define a parameter table of Relative Position Bias\n",
        "        self.rpb_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), n_heads)   # (2*Wh-1 * 2*Ww-1, nH)\n",
        "        )\n",
        "\n",
        "        # constructing Relative Position Bias index matrix\n",
        "        # absolute coords:\n",
        "        x_coords = torch.arange(window_size[0])\n",
        "        y_coords = torch.arange(window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid(x_coords, y_coords, indexing='ij'), axis=0)\n",
        "        coords_flat = coords.flatten(start_dim=1)\n",
        "        # relative coords:\n",
        "        coords_rel = coords_flat.unsqueeze(2) - coords_flat.unsqueeze(1)\n",
        "        coords_rel = coords_rel.permute(1, 2, 0).contiguous()\n",
        "        coords_rel[:, :, 0] += window_size[0] - 1\n",
        "        coords_rel[:, :, 1] += window_size[1] - 1\n",
        "        coords_rel[:, :, 0] *= 2 * window_size[1] - 1\n",
        "        self.relative_position_index = coords_rel.sum(-1)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.permute(0, 3, 1, 2)               # -> (bW, C, Wh, Ww)\n",
        "        bW, c, Wh, Ww = x.shape                 # bW = b * nW\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)    # -> 3 x (bW, C, Wh, Ww)\n",
        "\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h d) x y -> b h d (x y)\", h=self.n_heads), qkv\n",
        "        )\n",
        "        q = q * self.qk_scale    # -> (bW, nH, C, Wh*Ww)  for q, k, v\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)    # -> (bW, nH, Wh*Ww, Wh*Ww)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "\n",
        "        rpb = self.rpb_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1   # -> (Wh*Ww, Wh*Ww, nH)\n",
        "        )\n",
        "\n",
        "        rpb = rpb.permute(2, 0, 1).contiguous()           # -> (nH, Wh*Ww, Wh*Ww)\n",
        "        sim = sim + rpb.unsqueeze(0)                      # rpb -> (1, nH, Wh*Ww, Wh*Ww)\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]                            # -> n of windows in single img\n",
        "            sim = rearrange(sim, \"(b nW) nH i j -> b nW nH i j\", nW=nW)\n",
        "            sim = sim + mask.unsqueeze(1).unsqueeze(0)    # mask -> (1, nW, 1, Wh*Ww, Wh*Ww)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        if mask is not None:\n",
        "            attn = rearrange(attn, \"b nW nH i j -> (b nW) nH i j\")\n",
        "\n",
        "        output = einsum(\"b h i j, b h d j -> b h i d\", attn, v)                 # -> (bW, nH, Wh*Ww, C)\n",
        "        output = rearrange(output, \"b h (x y) d -> b (h d) x y\", x=Wh, y=Ww)    # -> (bW, C, Wh, Ww)\n",
        "        output = self.to_out(output)\n",
        "        output = output.permute(0, 2, 3, 1)                                     # -> (bW, Wh, Ww, C)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ss2AHuNOQ3Vo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WMSA check\n",
        "\n",
        "# mock mask\n",
        "mmask = torch.ones(8, 4, 4)\n",
        "mmask.masked_fill(mmask != 0, float(-100.0)).masked_fill(mmask == 0, float(0.0))\n",
        "\n",
        "t = torch.arange(96).view(2, 4, 4, 3).to(torch.float)\n",
        "w = window_partition(t, window_size=2)\n",
        "print(f'{w.shape=}')\n",
        "wattn = WMSA(input_dim=3, window_size=(2, 2), n_heads=1)\n",
        "res = wattn(w, mmask)\n",
        "# res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnjqxg93Vluj",
        "outputId": "3868e056-ea80-4056-f5a3-5ccb1191ee0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w.shape=torch.Size([8, 2, 2, 3])\n",
            "attn shape final torch.Size([1, 8, 1, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.randn(2, 16)  # 16 = 4 * 4\n",
        "(t.unsqueeze(1) - t.unsqueeze(2)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnnq9OAUi2_2",
        "outputId": "ea8a9538-2928-4a20-afb9-c8ec491a494d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relative Position Bias\n",
        "Wh = 4\n",
        "Ww = 4\n",
        "window_size = (Wh, Ww)\n",
        "\n",
        "x_coords = torch.arange(window_size[0])\n",
        "y_coords = torch.arange(window_size[1])\n",
        "coords = torch.stack(torch.meshgrid(x_coords, y_coords), axis=0)\n",
        "coords_flat = coords.flatten(start_dim=1)\n",
        "coords_rel = coords_flat.unsqueeze(2) - coords_flat.unsqueeze(1)\n",
        "coords_rel = coords_rel.permute(1, 2, 0).contiguous()\n",
        "coords_rel[:, :, 0] += window_size[0] - 1\n",
        "coords_rel[:, :, 1] += window_size[1] - 1\n",
        "coords_rel[:, :, 0] *= 2 * window_size[1] - 1\n",
        "relative_position_index = coords_rel.sum(-1)\n",
        "\n",
        "relative_position_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FLMypl2yBE3",
        "outputId": "932d7122-0e0e-4edf-8b1e-68f8e5a06a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# qkv -> [torch.Size([8, 3, 2, 2]), torch.Size([8, 3, 2, 2]), torch.Size([8, 3, 2, 2])]"
      ],
      "metadata": {
        "id": "WuMZosZAZ54G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mask(resolution, winsize, shift):\n",
        "    \"\"\"\n",
        "    Constructs boolean mask for WMSA.\n",
        "\n",
        "    Parameters:\n",
        "        resolution (Tuple[int, int]): height and width of input tensor\n",
        "        winsize (int): size of window\n",
        "        shift (int): number of patches to move up and left\n",
        "    \"\"\"\n",
        "    h, w = resolution\n",
        "    mask = torch.zeros((1, h, w, 1))\n",
        "\n",
        "    h_stamps = (0, -winsize, -shift)\n",
        "    w_stamps = (0, -winsize, -shift)\n",
        "\n",
        "    for hs in h_stamps:\n",
        "        mask[:, hs:, :, :] += 1\n",
        "    for ws in w_stamps:\n",
        "        mask[:, :, ws:, :] += 3\n",
        "\n",
        "    mask = window_partition(mask, window_size=winsize)    # -> (nW, Wh, Ww, 1)\n",
        "    mask = mask.flatten(start_dim=1)                      # -> (nW, Wh*Ww)\n",
        "    attn_mask = mask.unsqueeze(1) - mask.unsqueeze(2)     # -> (nW, (Wh*ww)*(Wh*Ww))\n",
        "    attn_mask = (attn_mask != 0).float() * -100.0\n",
        "\n",
        "    return attn_mask"
      ],
      "metadata": {
        "id": "j0JwuU52oSD9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer Block\n",
        "\n",
        "    Parameters:\n",
        "        input_dim (int): number of channels in input tensor\n",
        "        input_resolution (Tuple[int]): input resulotion\n",
        "        n_heads (int): number of attentions heads\n",
        "        window_size (int): window size\n",
        "        shift_size (int): shift size for SW-MSA.\n",
        "        mlp_ratio (float): ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        mlp_drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, input_resolution, n_heads,\n",
        "                 window_size=7, shift_size=2, mlp_ratio=4., qkv_bias=True,\n",
        "                 qk_scale=None, mlp_drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.input_resolution = input_resolution    # нужно передать, чтобы создать маску без входной картинки\n",
        "        self.n_heads = n_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # if window is larger than the image - we work with only one window and do no shifts\n",
        "        if min(self.input_resolution) < self.window_size:\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "\n",
        "        self.norm1 = norm_layer(input_dim)\n",
        "        self.attn = WMSA(input_dim=input_dim, window_size=(window_size, window_size),\n",
        "                         n_heads=n_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                         attn_drop=attn_drop, output_drop=mlp_drop)\n",
        "        self.norm2 = norm_layer(input_dim)\n",
        "        mlp_hidden_dim = int(input_dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=input_dim, hid_features=mlp_hidden_dim,\n",
        "                       act_layer=act_layer, drop=mlp_drop)\n",
        "\n",
        "\n",
        "        # building mask\n",
        "        if self.shift_size > 0:\n",
        "            self.attn_mask = build_mask(resolution=self.input_resolution,\n",
        "                                        winsize=self.window_size,\n",
        "                                        shift=self.shift_size)\n",
        "        else:\n",
        "            self.attn_mask = None\n",
        "\n",
        "        self.drop_path = DropPath(drop_path)\n",
        "\n",
        "    def forward(self, x):    # x -> (B, H, W, C)\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        residual = x\n",
        "        x = self.norm1(x.view(B, H*W, C))\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x_shifted = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x_shifted = x\n",
        "\n",
        "        # window partition\n",
        "        x_windows = window_partition(x_shifted, self.window_size)    # -> (bW, Wh, Ww, C)\n",
        "\n",
        "        # WMSA with shifts\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)     # -> (bW, Wh, Ww, C)\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x_shifted = window_merging(attn_windows, self.window_size, H, W)    # -> (B, H, W, C)\n",
        "            x = torch.roll(x_shifted, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = window_merging(attn_windows, self.window_size, H, W)    # -> (B, H, W, C)\n",
        "            x = shifted_x\n",
        "\n",
        "        x = x + residual\n",
        "        x = x.view(B, H * W, C)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "rXR8VIzmRMpJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tens = torch.randn((6, 32, 32, 3))\n",
        "\n",
        "pe = PatchEmbedding(patch_size=4, emb_dim=48)\n",
        "p_embs = pe(tens)\n",
        "print(f'{p_embs.shape=}')    # -> (2, 8, 8, 16)\n",
        "\n",
        "swt = SwinTransformerBlock(\n",
        "    input_dim=48, input_resolution=(8, 8), n_heads=6,\n",
        "    window_size=4, shift_size=2\n",
        ")\n",
        "\n",
        "swt_out = swt(p_embs)\n",
        "print(f'{swt_out.shape=}')"
      ],
      "metadata": {
        "id": "7IWQb2VS9aEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309db303-849f-4ea8-ed10-c63fe61a8514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_embs.shape=torch.Size([6, 8, 8, 48])\n",
            "qkv[0].shape=torch.Size([24, 48, 4, 4])\n",
            "swt_out.shape=torch.Size([6, 8, 8, 48])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4) Patch merging"
      ],
      "metadata": {
        "id": "dsB8zbdXgnYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch Merging (downsampling method)\n",
        "\n",
        "    Parameters:\n",
        "        input_dim (int): number of channels in input tensor\n",
        "        norm_layer (nn.Module, optional): normalization to apply before merging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.reduction = nn.Conv2d(input_dim, 2 * input_dim, kernel_size=2, stride=2, bias=False)\n",
        "        self.norm_layer = norm_layer(input_dim)\n",
        "\n",
        "    def forward(self, x):    # x -> (B, H, W, C)\n",
        "        x = self.norm_layer(x)\n",
        "        x = x.permute(0, 3, 1, 2)   # -> (B, C, H, W)\n",
        "        x = self.reduction(x)\n",
        "        x = x.permute(0, 2, 3, 1)   # -> (B, H, W, C)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "O0Oc6QEYtKDI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tens = torch.randn((6, 32, 32, 3))\n",
        "pe = PatchEmbedding(patch_size=4, emb_dim=48)\n",
        "p_embs = pe(tens)\n",
        "\n",
        "swt = SwinTransformerBlock(\n",
        "    input_dim=48, input_resolution=(8, 8), n_heads=6,\n",
        "    window_size=4, shift_size=2\n",
        ")\n",
        "\n",
        "swt_out = swt(p_embs)\n",
        "print(f'{swt_out.shape=}')\n",
        "\n",
        "pm = PatchMerging(input_dim=48)\n",
        "pm_out = pm(swt_out)\n",
        "print(f'{pm_out.shape=}')"
      ],
      "metadata": {
        "id": "8xVRhqevtO94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b6453e-5eda-4e7c-8200-319d6dbf795c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qkv[0].shape=torch.Size([24, 48, 4, 4])\n",
            "swt_out.shape=torch.Size([6, 8, 8, 48])\n",
            "pm_out.shape=torch.Size([6, 4, 4, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5) Basic Layer -> full arch"
      ],
      "metadata": {
        "id": "OLNKbrpNolav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic Swin Transformer layer.\n",
        "    Stacks several Swin Blocks and puts the downsampler afterwords.\n",
        "\n",
        "    Parameters:\n",
        "        input_dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        n_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of MLP hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        mlp_drop (float, optional): MLP dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, input_resolution, depth, n_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, mlp_drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "\n",
        "        # SWIN blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(\n",
        "                input_dim=input_dim, input_resolution=input_resolution,\n",
        "                n_heads=n_heads, window_size=window_size,\n",
        "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                mlp_drop=mlp_drop, attn_drop=attn_drop,\n",
        "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        # downsampling\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(input_dim=input_dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "vbWiNCtPbyNC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BasicLayer checking\n",
        "tens = torch.randn((6, 64, 64, 3))\n",
        "pe = PatchEmbedding(patch_size=4, emb_dim=48)\n",
        "pm = PatchMerging(input_dim=48)\n",
        "\n",
        "p_embs = pe(tens)\n",
        "print(f'{p_embs.shape=}')\n",
        "\n",
        "bl = BasicLayer(\n",
        "    input_dim=48, input_resolution=(16, 16), depth=3,\n",
        "    n_heads=6, window_size=4, downsample=PatchMerging\n",
        ")\n",
        "\n",
        "blayer_out = bl(p_embs)\n",
        "print(f'{blayer_out.shape=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqQxIyoehFE8",
        "outputId": "2c84c10f-310b-451f-e453-550402638b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_embs.shape=torch.Size([6, 16, 16, 48])\n",
            "blayer_out.shape=torch.Size([6, 8, 8, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    SWIN Transformer\n",
        "\n",
        "    Parameters:\n",
        "        img_size (int): image size\n",
        "        patch_size (int): patch size\n",
        "        input_dim (int): number of channels in input image\n",
        "        n_classes (int): number of classes for classification\n",
        "        emb_dim (int): embedding dimension\n",
        "        depths (List[int]): number of Swin Blocks in each Basic Layer\n",
        "        n_heads (List[int]): number of attn heads in each Basic Layer\n",
        "        window_size (int): window size\n",
        "        mlp_ratio (float): ratio of MLP hidden dim to embedding dim\n",
        "        qkv_bias (bool, optional): if True, add a learnable bias to query, key, value\n",
        "        qk_scale (float | None, optional): override default qk scale of head_dim ** -0.5 if set\n",
        "        mlp_drop (float, optional): MLP dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): normalization to apply after feature extraction\n",
        "        patch_norm (nn.Module, optional): normalization for PatchEmbedding layer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, input_dim=3, n_classes=1000,\n",
        "                 emb_dim=96, depths=[2, 2, 6, 2], n_heads=[3, 6, 12, 24],\n",
        "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 mlp_drop=0., attn_drop=0., drop_path=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = len(depths)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = int(emb_dim * 2 ** (self.n_layers - 1))\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(patch_size=patch_size, emb_dim=emb_dim,\n",
        "                                          norm_layer=norm_layer if self.patch_norm else None)\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "\n",
        "        # stochastic depth decay\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]\n",
        "\n",
        "        # basic layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            BasicLayer(dim=int(emb_dim * 2 ** i),\n",
        "                        input_resolution=(patches_resolution[0] // (2 ** i),\n",
        "                                          patches_resolution[1] // (2 ** i)),\n",
        "                        depth=depths[i],\n",
        "                        num_heads=n_heads[i],\n",
        "                        window_size=window_size,\n",
        "                        mlp_ratio=self.mlp_ratio,\n",
        "                        qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                        mlp_drop=mlp_drop, attn_drop=attn_drop,\n",
        "                        drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
        "                        norm_layer=norm_layer,\n",
        "                        downsample=PatchMerging if (i < self.n_layers - 1) else None)\n",
        "            for i in range(self.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        # self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = nn.Linear(self.num_features, n_classes) if n_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        # x = self.avgpool(x.transpose(1, 2))\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extract_features(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        # if isinstance(m, nn.Linear):\n",
        "        #     trunc_normal_(m.weight, std=.02)\n",
        "        #     if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "        #         nn.init.constant_(m.bias, 0)\n",
        "        # elif isinstance(m, nn.LayerNorm):\n",
        "        #     nn.init.constant_(m.bias, 0)\n",
        "        #     nn.init.constant_(m.weight, 1.0)\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "_TQRII7gm5UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgImRyS-YERI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trials:"
      ],
      "metadata": {
        "id": "0MwXUEJsX-ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mask(resolution, winsize, shift):\n",
        "    h, w = resolution\n",
        "    mask = torch.zeros((1, h, w, 1))\n",
        "\n",
        "    # h_stamps = (0, -winsize, -shift, None)\n",
        "    # w_stamps = (0, -winsize, -shift, None)\n",
        "    # h_slices = [slice(*x) for x in zip(h_stamps[:-1], h_stamps[1:])]\n",
        "    # w_slices = [slice(*x) for x in zip(w_stamps[:-1], w_stamps[1:])]\n",
        "\n",
        "    # h_slices = (\n",
        "    #     slice(0, -winsize),\n",
        "    #     slice(-winsize, -shift),\n",
        "    #     slice(-shift, None)\n",
        "    # )\n",
        "\n",
        "    # w_slices = (\n",
        "    #     slice(0, -winsize),\n",
        "    #     slice(-winsize, -shift),\n",
        "    #     slice(-shift, None)\n",
        "    # )\n",
        "\n",
        "    # for hs in h_slices:\n",
        "    #     mask[:, hs, :, :] += 1\n",
        "    # for ws in w_slices:\n",
        "    #     mask[:, :, ws, :] += 1\n",
        "\n",
        "    h_stamps = (0, -winsize, -shift)\n",
        "    w_stamps = (0, -winsize, -shift)\n",
        "    for hs in h_stamps:\n",
        "        mask[:, hs:, :, :] += 1\n",
        "    for ws in w_stamps:\n",
        "        mask[:, :, ws:, :] += 3\n",
        "\n",
        "    mask = window_partition(mask, window_size=winsize)    # -> (nW, Wh, Ww, 1)\n",
        "    mask = mask.flatten(start_dim=1)                      # -> (nW, Wh*Ww)\n",
        "    attn_mask = mask.unsqueeze(1) - mask.unsqueeze(2)     # -> (nW, (Wh*ww)*(Wh*Ww))\n",
        "    attn_mask = (attn_mask != 0).float() * -100.0\n",
        "\n",
        "    return attn_mask"
      ],
      "metadata": {
        "id": "No8olssnN_5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # numbers:\n",
        "# tensor([[ 4.,  4.,  4.,  4.,  7.,  7., 10., 10.],\n",
        "#         [ 4.,  4.,  4.,  4.,  7.,  7., 10., 10.],\n",
        "#         [ 4.,  4.,  4.,  4.,  7.,  7., 10., 10.],\n",
        "#         [ 4.,  4.,  4.,  4.,  7.,  7., 10., 10.],\n",
        "#         [ 5.,  5.,  5.,  5.,  8.,  8., 11., 11.],\n",
        "#         [ 5.,  5.,  5.,  5.,  8.,  8., 11., 11.],\n",
        "#         [ 6.,  6.,  6.,  6.,  9.,  9., 12., 12.],\n",
        "#         [ 6.,  6.,  6.,  6.,  9.,  9., 12., 12.]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXP2bTSjUbTB",
        "outputId": "36ea816c-4ada-4af6-c5cc-3201fe9b8467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[slice(0, -4, None), slice(-4, -2, None), slice(-2, None, None)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
        "    if keep_prob > 0.0 and scale_by_keep:\n",
        "        random_tensor.div_(keep_prob)\n",
        "\n",
        "    return x * random_tensor"
      ],
      "metadata": {
        "id": "427Y_5LoVOnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-0 == 0"
      ],
      "metadata": {
        "id": "-W-m1wsMjV0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c4fc88-2471-463a-b559-0b4e074f310e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = 5\n",
        "b = 6\n",
        "a = b or c\n",
        "\n",
        "a"
      ],
      "metadata": {
        "id": "2IcRwH4Rm12_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d9e994-466b-407d-8f1f-efacd762aaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjuY2ww2Zz_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}