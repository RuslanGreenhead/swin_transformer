# Config for SwinUnetDiffusion for MNIST generation
model:
  swin:
    emb_dim: 96
    depths: [2, 2, 6, 2]
    n_heads: [3, 6, 12, 24]
    mlp_drop: 0.0
    attn_drop: 0.0
    drop_path: 0.2
  pretrained_weights: ~ # "IMAGENET_CUSTOM"
  image_size: 224
  n_classes: 10            # without "background"
  sync_batchnorm: False    # syncrhronize BN calculation across all GPUs
  out_dim: 1               # 1 is inefficient, supposely
  mid_depth: 2
  time_emb_dim: 128
  condition: True
  condition_emb_dim: 32      # initial embedding acquired via nn.Embedding table
  condition_dim: 4           # filal channel dim of condition tensor

train:
  optimizer: "adam"
  num_epochs: 75          
  warmup_epochs: 3              # (5 for 100 epochs, 3 for 75 - (~5%))
  accumulation_steps: 1 
  warmup_prefix: True
  initial_lr: 0.0001      # 1e-4
  min_lr: 0.000005        # 5e-6
  warmup_lr: 0.00000001    # 1e-7  (1e-8 for SSD with neck)
  max_grad_norm: 5.0      # as in authors' code
  weight_decay: 0.0001    # recommended for diffusion (instead of standard 0.05)

  n_timesteps: 500         # diffusion timesteps
  
  batch_size: 64           # (2 gpus) * (64 per each) --> 128 (total)  | maybe should increase to ...
  batch_interval: 100      # we have ~390 batches  
  sampling_interval: 5     # frequency (in epochs) of generating samples
  num_workers: 8           # 8 regardless of number of CPUs/GPUs
  save_every: 15
  snapshot_path: 'model_snapshot.pth'
  samples_path: 'generated'

dataset:
  name: "MNIST"                            # or "CIFAR-10"
  root: "../my_data/mnist_train_raw.pth"   # or "../my_data/cifar10_train_raw.pth" 
  image_size: ~
  n_classes: 10        